digraph {
	graph [size="12.45,12.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2050920767632 [label="
 ()" fillcolor=darkolivegreen1]
	2050920748512 [label=MeanBackward0]
	2050920749520 -> 2050920748512
	2050920749520 [label=AddmmBackward0]
	2050920749376 -> 2050920749520
	2052939295296 [label="dense_layer14.bias
 (1)" fillcolor=lightblue]
	2052939295296 -> 2050920749376
	2050920749376 [label=AccumulateGrad]
	2050920749232 -> 2050920749520
	2050920749232 [label=ReluBackward0]
	2050920748560 -> 2050920749232
	2050920748560 [label=MmBackward0]
	2050920749136 -> 2050920748560
	2050920749136 [label=LeakyReluBackward0]
	2050920748992 -> 2050920749136
	2050920748992 [label=MmBackward0]
	2050920748896 -> 2050920748992
	2050920748896 [label=ReshapeAliasBackward0]
	2050920748752 -> 2050920748896
	2050920748752 [label=LeakyReluBackward0]
	2050920748656 -> 2050920748752
	2050920748656 [label=ConvolutionBackward0]
	2050920748416 -> 2050920748656
	2050920748416 [label=LeakyReluBackward0]
	2050920748224 -> 2050920748416
	2050920748224 [label=ConvolutionBackward0]
	2050920748128 -> 2050920748224
	2050920748128 [label=LeakyReluBackward0]
	2050920747840 -> 2050920748128
	2050920747840 [label=ConvolutionBackward0]
	2050920747696 -> 2050920747840
	2050920747696 [label=ConvolutionBackward0]
	2050920749616 -> 2050920747696
	2052939296096 [label="conv_layer0.weight
 (32, 11, 5)" fillcolor=lightblue]
	2052939296096 -> 2050920749616
	2050920749616 [label=AccumulateGrad]
	2050920749568 -> 2050920747696
	2052939296016 [label="conv_layer0.bias
 (32)" fillcolor=lightblue]
	2052939296016 -> 2050920749568
	2050920749568 [label=AccumulateGrad]
	2050920747792 -> 2050920747840
	2052939295936 [label="conv_layer1.weight
 (64, 32, 5)" fillcolor=lightblue]
	2052939295936 -> 2050920747792
	2050920747792 [label=AccumulateGrad]
	2050920747936 -> 2050920747840
	2052939295856 [label="conv_layer1.bias
 (64)" fillcolor=lightblue]
	2052939295856 -> 2050920747936
	2050920747936 [label=AccumulateGrad]
	2050920748176 -> 2050920748224
	2052939296176 [label="conv_layer3.weight
 (128, 64, 5)" fillcolor=lightblue]
	2052939296176 -> 2050920748176
	2050920748176 [label=AccumulateGrad]
	2050920748320 -> 2050920748224
	2052939295776 [label="conv_layer3.bias
 (128)" fillcolor=lightblue]
	2052939295776 -> 2050920748320
	2050920748320 [label=AccumulateGrad]
	2050920748608 -> 2050920748656
	2052939295696 [label="conv_layer6.weight
 (256, 128, 5)" fillcolor=lightblue]
	2052939295696 -> 2050920748608
	2050920748608 [label=AccumulateGrad]
	2050920748848 -> 2050920748656
	2052939295616 [label="conv_layer6.bias
 (256)" fillcolor=lightblue]
	2052939295616 -> 2050920748848
	2050920748848 [label=AccumulateGrad]
	2050920748944 -> 2050920748992
	2050920748944 [label=TBackward0]
	2050920748368 -> 2050920748944
	2052939295536 [label="dense_layer9.weight
 (256, 256)" fillcolor=lightblue]
	2052939295536 -> 2050920748368
	2050920748368 [label=AccumulateGrad]
	2050920749184 -> 2050920748560
	2050920749184 [label=TBackward0]
	2050920748704 -> 2050920749184
	2052939295456 [label="dense_layer12.weight
 (256, 256)" fillcolor=lightblue]
	2052939295456 -> 2050920748704
	2050920748704 [label=AccumulateGrad]
	2050920749472 -> 2050920749520
	2050920749472 [label=TBackward0]
	2050920749088 -> 2050920749472
	2052939295376 [label="dense_layer14.weight
 (1, 256)" fillcolor=lightblue]
	2052939295376 -> 2050920749088
	2050920749088 [label=AccumulateGrad]
	2050920748512 -> 2050920767632
}
