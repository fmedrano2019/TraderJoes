digraph {
	graph [size="12.45,12.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2133347732192 [label="
 ()" fillcolor=darkolivegreen1]
	2133347741808 [label=MeanBackward0]
	2133347741760 -> 2133347741808
	2133347741760 [label=AddmmBackward0]
	2133347255296 -> 2133347741760
	2133153316928 [label="dense_layer14.bias
 (1)" fillcolor=lightblue]
	2133153316928 -> 2133347255296
	2133347255296 [label=AccumulateGrad]
	2133301961008 -> 2133347741760
	2133301961008 [label=ReluBackward0]
	2133347651440 -> 2133301961008
	2133347651440 [label=MmBackward0]
	2133347650912 -> 2133347651440
	2133347650912 [label=LeakyReluBackward0]
	2133347093760 -> 2133347650912
	2133347093760 [label=MmBackward0]
	2133347687488 -> 2133347093760
	2133347687488 [label=ReshapeAliasBackward0]
	2133347684416 -> 2133347687488
	2133347684416 [label=LeakyReluBackward0]
	2133347685712 -> 2133347684416
	2133347685712 [label=ConvolutionBackward0]
	2133347687824 -> 2133347685712
	2133347687824 [label=LeakyReluBackward0]
	2133347685664 -> 2133347687824
	2133347685664 [label=ConvolutionBackward0]
	2133347686768 -> 2133347685664
	2133347686768 [label=LeakyReluBackward0]
	2133347686864 -> 2133347686768
	2133347686864 [label=ConvolutionBackward0]
	2133347686720 -> 2133347686864
	2133347686720 [label=ConvolutionBackward0]
	2133347580848 -> 2133347686720
	2133153317808 [label="conv_layer0.weight
 (32, 11, 5)" fillcolor=lightblue]
	2133153317808 -> 2133347580848
	2133347580848 [label=AccumulateGrad]
	2133347581616 -> 2133347686720
	2133153317728 [label="conv_layer0.bias
 (32)" fillcolor=lightblue]
	2133153317728 -> 2133347581616
	2133347581616 [label=AccumulateGrad]
	2133347581136 -> 2133347686864
	2133153317488 [label="conv_layer1.weight
 (64, 32, 5)" fillcolor=lightblue]
	2133153317488 -> 2133347581136
	2133347581136 [label=AccumulateGrad]
	2133347579648 -> 2133347686864
	2133153317408 [label="conv_layer1.bias
 (64)" fillcolor=lightblue]
	2133153317408 -> 2133347579648
	2133347579648 [label=AccumulateGrad]
	2133347507648 -> 2133347685664
	2133153317648 [label="conv_layer3.weight
 (128, 64, 5)" fillcolor=lightblue]
	2133153317648 -> 2133347507648
	2133347507648 [label=AccumulateGrad]
	2133347578208 -> 2133347685664
	2133153317568 [label="conv_layer3.bias
 (128)" fillcolor=lightblue]
	2133153317568 -> 2133347578208
	2133347578208 [label=AccumulateGrad]
	2133347650288 -> 2133347685712
	2133153317328 [label="conv_layer6.weight
 (256, 128, 5)" fillcolor=lightblue]
	2133153317328 -> 2133347650288
	2133347650288 [label=AccumulateGrad]
	2133347535936 -> 2133347685712
	2133153317248 [label="conv_layer6.bias
 (256)" fillcolor=lightblue]
	2133153317248 -> 2133347535936
	2133347535936 [label=AccumulateGrad]
	2133347687152 -> 2133347093760
	2133347687152 [label=TBackward0]
	2133347533248 -> 2133347687152
	2133153317168 [label="dense_layer9.weight
 (256, 256)" fillcolor=lightblue]
	2133153317168 -> 2133347533248
	2133347533248 [label=AccumulateGrad]
	2133347651104 -> 2133347651440
	2133347651104 [label=TBackward0]
	2133347507024 -> 2133347651104
	2133153317088 [label="dense_layer12.weight
 (256, 256)" fillcolor=lightblue]
	2133153317088 -> 2133347507024
	2133347507024 [label=AccumulateGrad]
	2133301516176 -> 2133347741760
	2133301516176 [label=TBackward0]
	2133347651344 -> 2133301516176
	2133153317008 [label="dense_layer14.weight
 (1, 256)" fillcolor=lightblue]
	2133153317008 -> 2133347651344
	2133347651344 [label=AccumulateGrad]
	2133347741808 -> 2133347732192
}
