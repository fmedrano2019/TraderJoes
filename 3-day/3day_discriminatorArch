digraph {
	graph [size="12.45,12.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	3128976215632 [label="
 ()" fillcolor=darkolivegreen1]
	3128976270528 [label=MeanBackward0]
	3128976270144 -> 3128976270528
	3128976270144 [label=AddmmBackward0]
	3128975768064 -> 3128976270144
	3130677041168 [label="dense_layer14.bias
 (1)" fillcolor=lightblue]
	3130677041168 -> 3128975768064
	3128975768064 [label=AccumulateGrad]
	3128975768208 -> 3128976270144
	3128975768208 [label=ReluBackward0]
	3128976704368 -> 3128975768208
	3128976704368 [label=MmBackward0]
	3128976704272 -> 3128976704368
	3128976704272 [label=LeakyReluBackward0]
	3128976706432 -> 3128976704272
	3128976706432 [label=MmBackward0]
	3128976706624 -> 3128976706432
	3128976706624 [label=ReshapeAliasBackward0]
	3128976703648 -> 3128976706624
	3128976703648 [label=LeakyReluBackward0]
	3128976706384 -> 3128976703648
	3128976706384 [label=ConvolutionBackward0]
	3128974772784 -> 3128976706384
	3128974772784 [label=LeakyReluBackward0]
	3128975271200 -> 3128974772784
	3128975271200 [label=ConvolutionBackward0]
	3129227445680 -> 3128975271200
	3129227445680 [label=LeakyReluBackward0]
	3128974356192 -> 3129227445680
	3128974356192 [label=ConvolutionBackward0]
	3128976676608 -> 3128974356192
	3128976676608 [label=ConvolutionBackward0]
	3128976675792 -> 3128976676608
	3130677041808 [label="conv_layer0.weight
 (32, 4, 5)" fillcolor=lightblue]
	3130677041808 -> 3128976675792
	3128976675792 [label=AccumulateGrad]
	3128976678768 -> 3128976676608
	3130677041728 [label="conv_layer0.bias
 (32)" fillcolor=lightblue]
	3130677041728 -> 3128976678768
	3128976678768 [label=AccumulateGrad]
	3128976677760 -> 3128974356192
	3130677041968 [label="conv_layer1.weight
 (64, 32, 5)" fillcolor=lightblue]
	3130677041968 -> 3128976677760
	3128976677760 [label=AccumulateGrad]
	3128976676176 -> 3128974356192
	3130677041888 [label="conv_layer1.bias
 (64)" fillcolor=lightblue]
	3130677041888 -> 3128976676176
	3128976676176 [label=AccumulateGrad]
	3129227445920 -> 3128975271200
	3130677041648 [label="conv_layer3.weight
 (128, 64, 5)" fillcolor=lightblue]
	3130677041648 -> 3129227445920
	3129227445920 [label=AccumulateGrad]
	3129217593696 -> 3128975271200
	3130677041568 [label="conv_layer3.bias
 (128)" fillcolor=lightblue]
	3130677041568 -> 3129217593696
	3129217593696 [label=AccumulateGrad]
	3128976706144 -> 3128976706384
	3130677041488 [label="conv_layer6.weight
 (256, 128, 5)" fillcolor=lightblue]
	3130677041488 -> 3128976706144
	3128976706144 [label=AccumulateGrad]
	3128976079744 -> 3128976706384
	3130677041408 [label="conv_layer6.bias
 (256)" fillcolor=lightblue]
	3130677041408 -> 3128976079744
	3128976079744 [label=AccumulateGrad]
	3128976704176 -> 3128976706432
	3128976704176 [label=TBackward0]
	3128976079696 -> 3128976704176
	3130677041328 [label="dense_layer9.weight
 (256, 256)" fillcolor=lightblue]
	3130677041328 -> 3128976079696
	3128976079696 [label=AccumulateGrad]
	3128976706192 -> 3128976704368
	3128976706192 [label=TBackward0]
	3128975738864 -> 3128976706192
	3130677041248 [label="dense_layer12.weight
 (256, 256)" fillcolor=lightblue]
	3130677041248 -> 3128975738864
	3128975738864 [label=AccumulateGrad]
	3128976273072 -> 3128976270144
	3128976273072 [label=TBackward0]
	3128976705232 -> 3128976273072
	3130677042048 [label="dense_layer14.weight
 (1, 256)" fillcolor=lightblue]
	3130677042048 -> 3128976705232
	3128976705232 [label=AccumulateGrad]
	3128976270528 -> 3128976215632
}
