{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d452a7",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d99bd8",
   "metadata": {},
   "source": [
    "The `Scweet` library is old, and uses a deprecated `Selenium` function `find_elements_by_xpath()`, therefore, we must install an older version of `Selenium 4.2.0`.\n",
    "\n",
    "This can be done by running the command `pip install selenium==4.2.0 --force-reinstall --user` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe94fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Scweet.scweet import scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8bca6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "KEYWORDS = [\"$AAPL\", \"AAPL\"]\n",
    "MAX_TWEETS = 100\n",
    "MAX_REPLIES = 10\n",
    "ORIGINAL_DIR = \"data/scraped_twitter/original\"\n",
    "PROCESSED_DIR = \"data/scraped_twitter/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5cb90",
   "metadata": {},
   "source": [
    "# Scraping the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78a10e10",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping on headless mode.\n",
      "looking for tweets between 2013-12-29 and 2013-12-30 ...\n",
      " path : https://twitter.com/search?q=($AAPL%20OR%20AAPL)%20until%3A2013-12-30%20since%3A2013-12-29%20&src=typed_query\n",
      "Tweet made at: 2013-12-29T06:17:53.000Z is found.\n",
      "Tweet made at: 2013-12-29T15:10:29.000Z is found.\n",
      "Tweet made at: 2013-12-29T02:04:02.000Z is found.\n",
      "Tweet made at: 2013-12-29T23:08:49.000Z is found.\n",
      "Tweet made at: 2013-12-29T00:34:30.000Z is found.\n",
      "Tweet made at: 2013-12-29T03:58:42.000Z is found.\n",
      "Tweet made at: 2013-12-29T16:08:02.000Z is found.\n",
      "scroll  1\n",
      "Tweet made at: 2013-12-29T19:56:38.000Z is found.\n",
      "Tweet made at: 2013-12-29T04:03:29.000Z is found.\n",
      "Tweet made at: 2013-12-29T16:16:03.000Z is found.\n",
      "Tweet made at: 2013-12-29T19:32:17.000Z is found.\n",
      "Tweet made at: 2013-12-29T18:12:10.000Z is found.\n",
      "Tweet made at: 2013-12-29T19:51:03.000Z is found.\n",
      "Tweet made at: 2013-12-29T17:19:42.000Z is found.\n",
      "Tweet made at: 2013-12-29T01:44:39.000Z is found.\n",
      "Tweet made at: 2013-12-29T18:45:13.000Z is found.\n",
      "Tweet made at: 2013-12-29T23:45:36.000Z is found.\n",
      "Tweet made at: 2013-12-29T00:14:44.000Z is found.\n",
      "Tweet made at: 2013-12-29T22:01:11.000Z is found.\n",
      "Tweet made at: 2013-12-29T23:20:17.000Z is found.\n",
      "scroll  2\n",
      "scroll  3\n",
      "scroll  4\n",
      "looking for tweets between 2013-12-30 and 2013-12-31 ...\n",
      " path : https://twitter.com/search?q=($AAPL%20OR%20AAPL)%20until%3A2013-12-31%20since%3A2013-12-30%20&src=typed_query\n",
      "Tweet made at: 2013-12-30T19:21:28.000Z is found.\n",
      "Tweet made at: 2013-12-30T22:27:35.000Z is found.\n",
      "Tweet made at: 2013-12-30T19:54:30.000Z is found.\n",
      "Tweet made at: 2013-12-30T21:24:13.000Z is found.\n",
      "Tweet made at: 2013-12-30T18:40:20.000Z is found.\n",
      "Tweet made at: 2013-12-30T18:06:44.000Z is found.\n",
      "Tweet made at: 2013-12-30T21:15:08.000Z is found.\n",
      "Tweet made at: 2013-12-30T22:14:21.000Z is found.\n",
      "Tweet made at: 2013-12-30T20:18:07.000Z is found.\n",
      "Tweet made at: 2013-12-30T11:50:06.000Z is found.\n",
      "Tweet made at: 2013-12-30T23:16:53.000Z is found.\n",
      "scroll  1\n",
      "Tweet made at: 2013-12-30T15:03:36.000Z is found.\n",
      "Tweet made at: 2013-12-30T07:40:55.000Z is found.\n",
      "Tweet made at: 2013-12-30T17:25:03.000Z is found.\n",
      "Tweet made at: 2013-12-30T15:55:02.000Z is found.\n",
      "Tweet made at: 2013-12-30T16:43:20.000Z is found.\n",
      "Tweet made at: 2013-12-30T19:31:08.000Z is found.\n",
      "Tweet made at: 2013-12-30T23:43:29.000Z is found.\n",
      "Tweet made at: 2013-12-30T17:20:59.000Z is found.\n",
      "scroll  2\n",
      "Tweet made at: 2013-12-30T05:42:16.000Z is found.\n",
      "Tweet made at: 2013-12-30T20:40:58.000Z is found.\n",
      "Tweet made at: 2013-12-30T03:18:11.000Z is found.\n",
      "Tweet made at: 2013-12-30T19:24:43.000Z is found.\n",
      "Tweet made at: 2013-12-30T16:39:19.000Z is found.\n",
      "Tweet made at: 2013-12-30T14:53:47.000Z is found.\n",
      "Tweet made at: 2013-12-30T14:39:49.000Z is found.\n",
      "Tweet made at: 2013-12-30T20:11:01.000Z is found.\n",
      "Tweet made at: 2013-12-30T21:37:28.000Z is found.\n",
      "Tweet made at: 2013-12-30T15:11:21.000Z is found.\n",
      "Tweet made at: 2013-12-30T16:43:13.000Z is found.\n",
      "Tweet made at: 2013-12-30T16:11:13.000Z is found.\n",
      "Tweet made at: 2013-12-30T14:12:49.000Z is found.\n",
      "Tweet made at: 2013-12-30T15:36:27.000Z is found.\n",
      "Tweet made at: 2013-12-30T14:21:22.000Z is found.\n",
      "Tweet made at: 2013-12-30T23:35:08.000Z is found.\n",
      "Tweet made at: 2013-12-30T17:42:30.000Z is found.\n",
      "scroll  3\n",
      "Tweet made at: 2013-12-30T13:52:16.000Z is found.\n",
      "Tweet made at: 2013-12-30T19:53:19.000Z is found.\n",
      "Tweet made at: 2013-12-30T16:30:24.000Z is found.\n",
      "Tweet made at: 2013-12-30T16:17:48.000Z is found.\n",
      "Tweet made at: 2013-12-30T15:45:57.000Z is found.\n",
      "Tweet made at: 2013-12-30T06:15:22.000Z is found.\n",
      "Tweet made at: 2013-12-30T14:01:19.000Z is found.\n",
      "Tweet made at: 2013-12-30T13:32:34.000Z is found.\n",
      "Tweet made at: 2013-12-30T17:46:30.000Z is found.\n",
      "Tweet made at: 2013-12-30T19:54:14.000Z is found.\n",
      "Tweet made at: 2013-12-30T01:06:34.000Z is found.\n",
      "Tweet made at: 2013-12-30T22:25:45.000Z is found.\n",
      "Tweet made at: 2013-12-30T16:22:35.000Z is found.\n",
      "Tweet made at: 2013-12-30T21:38:53.000Z is found.\n",
      "Tweet made at: 2013-12-30T20:43:05.000Z is found.\n",
      "Tweet made at: 2013-12-30T13:03:06.000Z is found.\n",
      "Tweet made at: 2013-12-30T19:45:40.000Z is found.\n",
      "Tweet made at: 2013-12-30T17:09:57.000Z is found.\n",
      "scroll  4\n",
      "Tweet made at: 2013-12-30T14:26:10.000Z is found.\n",
      "Tweet made at: 2013-12-30T16:13:55.000Z is found.\n",
      "Tweet made at: 2013-12-30T12:05:35.000Z is found.\n",
      "Tweet made at: 2013-12-30T16:30:24.000Z is found.\n",
      "Tweet made at: 2013-12-30T16:53:38.000Z is found.\n",
      "Tweet made at: 2013-12-30T14:23:35.000Z is found.\n",
      "Tweet made at: 2013-12-30T17:41:20.000Z is found.\n",
      "Tweet made at: 2013-12-30T13:26:30.000Z is found.\n",
      "Tweet made at: 2013-12-30T14:33:56.000Z is found.\n",
      "Tweet made at: 2013-12-30T19:40:34.000Z is found.\n",
      "Tweet made at: 2013-12-30T12:49:10.000Z is found.\n",
      "Tweet made at: 2013-12-30T11:30:47.000Z is found.\n",
      "Tweet made at: 2013-12-30T15:36:23.000Z is found.\n",
      "Tweet made at: 2013-12-30T14:22:05.000Z is found.\n",
      "Tweet made at: 2013-12-30T19:03:52.000Z is found.\n",
      "Tweet made at: 2013-12-30T19:38:52.000Z is found.\n",
      "Tweet made at: 2013-12-30T22:51:20.000Z is found.\n",
      "scroll  5\n",
      "Tweet made at: 2013-12-30T21:51:53.000Z is found.\n",
      "Tweet made at: 2013-12-30T03:19:10.000Z is found.\n",
      "Tweet made at: 2013-12-30T14:01:00.000Z is found.\n",
      "Tweet made at: 2013-12-30T21:25:53.000Z is found.\n",
      "Tweet made at: 2013-12-30T15:31:33.000Z is found.\n",
      "Tweet made at: 2013-12-30T01:59:30.000Z is found.\n",
      "Tweet made at: 2013-12-30T13:40:11.000Z is found.\n",
      "Tweet made at: 2013-12-30T17:10:12.000Z is found.\n",
      "Tweet made at: 2013-12-30T15:42:58.000Z is found.\n",
      "Tweet made at: 2013-12-30T15:19:58.000Z is found.\n",
      "Tweet made at: 2013-12-30T12:56:45.000Z is found.\n",
      "scroll  6\n",
      "scroll  7\n",
      "scroll  8\n",
      "looking for tweets between 2013-12-31 and 2014-01-01 ...\n",
      " path : https://twitter.com/search?q=($AAPL%20OR%20AAPL)%20until%3A2014-01-01%20since%3A2013-12-31%20&src=typed_query\n",
      "Tweet made at: 2013-12-31T15:46:41.000Z is found.\n",
      "Tweet made at: 2013-12-31T23:31:10.000Z is found.\n",
      "Tweet made at: 2013-12-31T15:02:39.000Z is found.\n",
      "scroll  1\n",
      "Tweet made at: 2013-12-31T17:26:07.000Z is found.\n",
      "Tweet made at: 2013-12-31T02:10:37.000Z is found.\n",
      "Tweet made at: 2013-12-31T19:21:28.000Z is found.\n",
      "Tweet made at: 2013-12-31T00:56:41.000Z is found.\n",
      "Tweet made at: 2013-12-31T14:40:41.000Z is found.\n",
      "Tweet made at: 2013-12-31T01:39:49.000Z is found.\n",
      "Tweet made at: 2013-12-31T16:32:54.000Z is found.\n",
      "Tweet made at: 2013-12-31T15:49:08.000Z is found.\n",
      "Tweet made at: 2013-12-31T02:38:47.000Z is found.\n",
      "Tweet made at: 2013-12-31T04:43:08.000Z is found.\n",
      "Tweet made at: 2013-12-31T15:19:37.000Z is found.\n",
      "scroll  2\n",
      "Tweet made at: 2013-12-31T18:12:00.000Z is found.\n",
      "Tweet made at: 2013-12-31T04:29:00.000Z is found.\n",
      "Tweet made at: 2013-12-31T19:05:53.000Z is found.\n",
      "Tweet made at: 2013-12-31T08:39:38.000Z is found.\n",
      "Tweet made at: 2013-12-31T22:10:25.000Z is found.\n",
      "Tweet made at: 2013-12-31T15:10:40.000Z is found.\n",
      "Tweet made at: 2013-12-31T21:37:02.000Z is found.\n",
      "Tweet made at: 2013-12-31T14:55:01.000Z is found.\n",
      "Tweet made at: 2013-12-31T14:39:25.000Z is found.\n",
      "Tweet made at: 2013-12-31T22:18:20.000Z is found.\n",
      "Tweet made at: 2013-12-31T01:39:57.000Z is found.\n",
      "Tweet made at: 2013-12-31T08:19:23.000Z is found.\n",
      "Tweet made at: 2013-12-31T17:30:49.000Z is found.\n",
      "Tweet made at: 2013-12-31T22:16:58.000Z is found.\n",
      "Tweet made at: 2013-12-31T22:23:58.000Z is found.\n",
      "scroll  3\n",
      "Tweet made at: 2013-12-31T21:43:01.000Z is found.\n",
      "Tweet made at: 2013-12-31T20:04:14.000Z is found.\n",
      "Tweet made at: 2013-12-31T15:34:37.000Z is found.\n",
      "Tweet made at: 2013-12-31T11:37:55.000Z is found.\n",
      "Tweet made at: 2013-12-31T14:22:34.000Z is found.\n",
      "Tweet made at: 2013-12-31T22:39:30.000Z is found.\n",
      "Tweet made at: 2013-12-31T17:13:56.000Z is found.\n",
      "Tweet made at: 2013-12-31T14:39:10.000Z is found.\n",
      "Tweet made at: 2013-12-31T16:06:27.000Z is found.\n",
      "Tweet made at: 2013-12-31T15:26:50.000Z is found.\n",
      "Tweet made at: 2013-12-31T13:50:59.000Z is found.\n",
      "Tweet made at: 2013-12-31T21:08:36.000Z is found.\n",
      "Tweet made at: 2013-12-31T16:35:52.000Z is found.\n",
      "Tweet made at: 2013-12-31T14:29:51.000Z is found.\n",
      "Tweet made at: 2013-12-31T21:31:35.000Z is found.\n",
      "Tweet made at: 2013-12-31T00:21:46.000Z is found.\n",
      "Tweet made at: 2013-12-31T14:56:43.000Z is found.\n",
      "Tweet made at: 2013-12-31T18:54:53.000Z is found.\n",
      "Tweet made at: 2013-12-31T04:03:32.000Z is found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet made at: 2013-12-31T15:03:37.000Z is found.\n",
      "scroll  4\n",
      "Tweet made at: 2013-12-31T14:01:20.000Z is found.\n",
      "Tweet made at: 2013-12-31T13:30:32.000Z is found.\n",
      "Tweet made at: 2013-12-31T22:14:31.000Z is found.\n",
      "Tweet made at: 2013-12-31T17:23:09.000Z is found.\n",
      "Tweet made at: 2013-12-31T15:01:13.000Z is found.\n",
      "Tweet made at: 2013-12-31T15:15:45.000Z is found.\n",
      "Tweet made at: 2013-12-31T18:04:45.000Z is found.\n",
      "Tweet made at: 2013-12-31T15:26:39.000Z is found.\n",
      "Tweet made at: 2013-12-31T18:11:02.000Z is found.\n",
      "Tweet made at: 2013-12-31T20:00:30.000Z is found.\n",
      "Tweet made at: 2013-12-31T18:02:10.000Z is found.\n",
      "Tweet made at: 2013-12-31T23:08:43.000Z is found.\n",
      "Tweet made at: 2013-12-31T15:08:59.000Z is found.\n",
      "Tweet made at: 2013-12-31T01:46:40.000Z is found.\n",
      "Tweet made at: 2013-12-31T14:00:05.000Z is found.\n",
      "Tweet made at: 2013-12-31T16:25:22.000Z is found.\n",
      "Tweet made at: 2013-12-31T18:48:34.000Z is found.\n",
      "Tweet made at: 2013-12-31T03:25:35.000Z is found.\n",
      "scroll  5\n",
      "Tweet made at: 2013-12-31T19:04:53.000Z is found.\n",
      "Tweet made at: 2013-12-31T16:00:03.000Z is found.\n",
      "Tweet made at: 2013-12-31T19:40:10.000Z is found.\n",
      "Tweet made at: 2013-12-31T02:08:04.000Z is found.\n",
      "Tweet made at: 2013-12-31T04:31:09.000Z is found.\n",
      "Tweet made at: 2013-12-31T20:59:29.000Z is found.\n",
      "Tweet made at: 2013-12-31T18:08:17.000Z is found.\n",
      "Tweet made at: 2013-12-31T19:25:34.000Z is found.\n",
      "Tweet made at: 2013-12-31T16:41:39.000Z is found.\n",
      "Tweet made at: 2013-12-31T04:18:05.000Z is found.\n",
      "Tweet made at: 2013-12-31T16:06:05.000Z is found.\n",
      "Tweet made at: 2013-12-31T19:52:45.000Z is found.\n",
      "Tweet made at: 2013-12-31T12:33:51.000Z is found.\n",
      "Tweet made at: 2013-12-31T23:10:36.000Z is found.\n",
      "Tweet made at: 2013-12-31T13:06:40.000Z is found.\n",
      "Tweet made at: 2013-12-31T12:35:32.000Z is found.\n",
      "Tweet made at: 2013-12-31T13:18:38.000Z is found.\n",
      "scroll  6\n",
      "scroll  7\n",
      "scroll  8\n"
     ]
    }
   ],
   "source": [
    "scraped_data = scrape(words=keywords, since=\"2022-12-29\", until=\"2023-01-01\", interval=1, \n",
    "                      save_images=False, limit=MAX_TWEETS, headless=True, proxy=None, save_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1de7e2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186, 11)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9baa7c41",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserScreenName</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text</th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Image link</th>\n",
       "      <th>Tweet URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FMC (,)</td>\n",
       "      <td>@FreeMrktCptlst</td>\n",
       "      <td>2013-12-29T06:17:53.000Z</td>\n",
       "      <td>FMC (,)\\n@FreeMrktCptlst\\n路\\nDec 29, 2013</td>\n",
       "      <td>$AAPL needs to run a contest that awards a $14...</td>\n",
       "      <td> </td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[https://pbs.twimg.com/media/Bcoc2YQIMAAwRE6?f...</td>\n",
       "      <td>https://twitter.com/FreeMrktCptlst/status/4171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>David Patrick -President- Fitzstock Charts LLC</td>\n",
       "      <td>@Fitzstock2004</td>\n",
       "      <td>2013-12-29T15:10:29.000Z</td>\n",
       "      <td>David Patrick -President- Fitzstock Charts LLC...</td>\n",
       "      <td>$AAPL update http://stks.co/f062X</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/Fitzstock2004/status/41731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rachel Shasha, MS, MFT</td>\n",
       "      <td>@Sassy_SPY</td>\n",
       "      <td>2013-12-29T02:04:02.000Z</td>\n",
       "      <td>Rachel Shasha, MS, MFT\\n@Sassy_SPY\\n路\\nDec 28,...</td>\n",
       "      <td>It's Over! Move Along + 1st OPEX of 2014 $SPY ...</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/Sassy_SPY/status/417113751...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analognotebook</td>\n",
       "      <td>@MonsaludJerry</td>\n",
       "      <td>2013-12-29T23:08:49.000Z</td>\n",
       "      <td>analognotebook\\n@MonsaludJerry\\n路\\nDec 29, 2013</td>\n",
       "      <td>what will keep pushing forward 2014?\\n$TSLA $G...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/MonsaludJerry/status/41743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daniel Eran Dilger</td>\n",
       "      <td>@DanielEran</td>\n",
       "      <td>2013-12-29T00:34:30.000Z</td>\n",
       "      <td>Daniel Eran Dilger\\n@DanielEran\\n路\\nDec 28, 2013</td>\n",
       "      <td>Editorial: 2013 was a terrible year for both A...</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/DanielEran/status/41709121...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   UserScreenName         UserName  \\\n",
       "0                                         FMC (,)  @FreeMrktCptlst   \n",
       "1  David Patrick -President- Fitzstock Charts LLC   @Fitzstock2004   \n",
       "2                          Rachel Shasha, MS, MFT       @Sassy_SPY   \n",
       "3                                  analognotebook   @MonsaludJerry   \n",
       "4                              Daniel Eran Dilger      @DanielEran   \n",
       "\n",
       "                  Timestamp  \\\n",
       "0  2013-12-29T06:17:53.000Z   \n",
       "1  2013-12-29T15:10:29.000Z   \n",
       "2  2013-12-29T02:04:02.000Z   \n",
       "3  2013-12-29T23:08:49.000Z   \n",
       "4  2013-12-29T00:34:30.000Z   \n",
       "\n",
       "                                                Text  \\\n",
       "0          FMC (,)\\n@FreeMrktCptlst\\n路\\nDec 29, 2013   \n",
       "1  David Patrick -President- Fitzstock Charts LLC...   \n",
       "2  Rachel Shasha, MS, MFT\\n@Sassy_SPY\\n路\\nDec 28,...   \n",
       "3    analognotebook\\n@MonsaludJerry\\n路\\nDec 29, 2013   \n",
       "4   Daniel Eran Dilger\\n@DanielEran\\n路\\nDec 28, 2013   \n",
       "\n",
       "                                       Embedded_text Emojis Comments Likes  \\\n",
       "0  $AAPL needs to run a contest that awards a $14...                   1   \n",
       "1                  $AAPL update http://stks.co/f062X                     1   \n",
       "2  It's Over! Move Along + 1st OPEX of 2014 $SPY ...               3     8   \n",
       "3  what will keep pushing forward 2014?\\n$TSLA $G...               1     1   \n",
       "4  Editorial: 2013 was a terrible year for both A...               9    13   \n",
       "\n",
       "  Retweets                                         Image link  \\\n",
       "0        1  [https://pbs.twimg.com/media/Bcoc2YQIMAAwRE6?f...   \n",
       "1        4                                                 []   \n",
       "2       11                                                 []   \n",
       "3        2                                                 []   \n",
       "4        8                                                 []   \n",
       "\n",
       "                                           Tweet URL  \n",
       "0  https://twitter.com/FreeMrktCptlst/status/4171...  \n",
       "1  https://twitter.com/Fitzstock2004/status/41731...  \n",
       "2  https://twitter.com/Sassy_SPY/status/417113751...  \n",
       "3  https://twitter.com/MonsaludJerry/status/41743...  \n",
       "4  https://twitter.com/DanielEran/status/41709121...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c781ec",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6abedcb",
   "metadata": {},
   "source": [
    "We first must keep only the `Timestamp` and `Embedded_text` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e05d7363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Embedded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-29T06:17:53.000Z</td>\n",
       "      <td>$AAPL needs to run a contest that awards a $14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-29T15:10:29.000Z</td>\n",
       "      <td>$AAPL update http://stks.co/f062X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-29T02:04:02.000Z</td>\n",
       "      <td>It's Over! Move Along + 1st OPEX of 2014 $SPY ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-29T23:08:49.000Z</td>\n",
       "      <td>what will keep pushing forward 2014?\\n$TSLA $G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-29T00:34:30.000Z</td>\n",
       "      <td>Editorial: 2013 was a terrible year for both A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2013-12-31T12:33:51.000Z</td>\n",
       "      <td>Summary of Yesterday's Webcast Featuring $AAPL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2013-12-31T23:10:36.000Z</td>\n",
       "      <td>Replying to \\n@SconsetCapital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2013-12-31T13:06:40.000Z</td>\n",
       "      <td>Early movers: HTZ, FDX, TWTR, NFLX, AAPL &amp; PSX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2013-12-31T12:35:32.000Z</td>\n",
       "      <td>***excellent piece, I agree 100%***\\n@jfahmy\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2013-12-31T13:18:38.000Z</td>\n",
       "      <td>Davis Polk on why the SEC rejected $AAPL attem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Timestamp  \\\n",
       "0    2013-12-29T06:17:53.000Z   \n",
       "1    2013-12-29T15:10:29.000Z   \n",
       "2    2013-12-29T02:04:02.000Z   \n",
       "3    2013-12-29T23:08:49.000Z   \n",
       "4    2013-12-29T00:34:30.000Z   \n",
       "..                        ...   \n",
       "181  2013-12-31T12:33:51.000Z   \n",
       "182  2013-12-31T23:10:36.000Z   \n",
       "183  2013-12-31T13:06:40.000Z   \n",
       "184  2013-12-31T12:35:32.000Z   \n",
       "185  2013-12-31T13:18:38.000Z   \n",
       "\n",
       "                                         Embedded_text  \n",
       "0    $AAPL needs to run a contest that awards a $14...  \n",
       "1                    $AAPL update http://stks.co/f062X  \n",
       "2    It's Over! Move Along + 1st OPEX of 2014 $SPY ...  \n",
       "3    what will keep pushing forward 2014?\\n$TSLA $G...  \n",
       "4    Editorial: 2013 was a terrible year for both A...  \n",
       "..                                                 ...  \n",
       "181  Summary of Yesterday's Webcast Featuring $AAPL...  \n",
       "182                      Replying to \\n@SconsetCapital  \n",
       "183  Early movers: HTZ, FDX, TWTR, NFLX, AAPL & PSX...  \n",
       "184  ***excellent piece, I agree 100%***\\n@jfahmy\\n...  \n",
       "185  Davis Polk on why the SEC rejected $AAPL attem...  \n",
       "\n",
       "[186 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = scraped_data[['Timestamp', 'Embedded_text']]\n",
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087902c",
   "metadata": {},
   "source": [
    "We then split up the `Timestamp` column into the date and time, for easier joining of the stock data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96dc2b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksnbx\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3641: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-29T06:17:53.000Z</td>\n",
       "      <td>$AAPL needs to run a contest that awards a $14...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>06:17:53.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-29T15:10:29.000Z</td>\n",
       "      <td>$AAPL update http://stks.co/f062X</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>15:10:29.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-29T02:04:02.000Z</td>\n",
       "      <td>It's Over! Move Along + 1st OPEX of 2014 $SPY ...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>02:04:02.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-29T23:08:49.000Z</td>\n",
       "      <td>what will keep pushing forward 2014?\\n$TSLA $G...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>23:08:49.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-29T00:34:30.000Z</td>\n",
       "      <td>Editorial: 2013 was a terrible year for both A...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>00:34:30.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Timestamp  \\\n",
       "0  2013-12-29T06:17:53.000Z   \n",
       "1  2013-12-29T15:10:29.000Z   \n",
       "2  2013-12-29T02:04:02.000Z   \n",
       "3  2013-12-29T23:08:49.000Z   \n",
       "4  2013-12-29T00:34:30.000Z   \n",
       "\n",
       "                                       Embedded_text        Date  \\\n",
       "0  $AAPL needs to run a contest that awards a $14...  2013-12-29   \n",
       "1                  $AAPL update http://stks.co/f062X  2013-12-29   \n",
       "2  It's Over! Move Along + 1st OPEX of 2014 $SPY ...  2013-12-29   \n",
       "3  what will keep pushing forward 2014?\\n$TSLA $G...  2013-12-29   \n",
       "4  Editorial: 2013 was a terrible year for both A...  2013-12-29   \n",
       "\n",
       "            Time  \n",
       "0  06:17:53.000Z  \n",
       "1  15:10:29.000Z  \n",
       "2  02:04:02.000Z  \n",
       "3  23:08:49.000Z  \n",
       "4  00:34:30.000Z  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sd.iloc[0].Timestamp\n",
    "sd[['Date', 'Time']] = sd['Timestamp'].str.split('T', expand=True)\n",
    "sd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4114fd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$AAPL needs to run a contest that awards a $14...</td>\n",
       "      <td>2013-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL update http://stks.co/f062X</td>\n",
       "      <td>2013-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's Over! Move Along + 1st OPEX of 2014 $SPY ...</td>\n",
       "      <td>2013-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what will keep pushing forward 2014?\\n$TSLA $G...</td>\n",
       "      <td>2013-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Editorial: 2013 was a terrible year for both A...</td>\n",
       "      <td>2013-12-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Embedded_text        Date\n",
       "0  $AAPL needs to run a contest that awards a $14...  2013-12-29\n",
       "1                  $AAPL update http://stks.co/f062X  2013-12-29\n",
       "2  It's Over! Move Along + 1st OPEX of 2014 $SPY ...  2013-12-29\n",
       "3  what will keep pushing forward 2014?\\n$TSLA $G...  2013-12-29\n",
       "4  Editorial: 2013 was a terrible year for both A...  2013-12-29"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping unnecessary columns and reordering\n",
    "sd = sd[['Embedded_text', 'Date']]\n",
    "sd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8612d69",
   "metadata": {},
   "source": [
    "We then clean the text using the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0ef4552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ksnbx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ksnbx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ksnbx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81cf6438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # break into tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # lowercase the text\n",
    "    lower = [word.lower() for word in tokens]\n",
    "    \n",
    "    # remove stopwords\n",
    "    no_stopwords = [word for word in lower if word not in stopword]\n",
    "    \n",
    "    # remove non-alphanumeric characters\n",
    "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
    "    \n",
    "    # lemmatize the tokens\n",
    "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
    "    \n",
    "    clean_text = lemm_text\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a17251",
   "metadata": {},
   "source": [
    "Thank you [Ona_Gilbert](https://www.kaggle.com/code/onadegibert/sentiment-analysis-with-tfidf-and-random-forest) for pointing us to the `nltk` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0707e4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksnbx\\AppData\\Local\\Temp/ipykernel_35084/3603823682.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sd['Cleaned_text'] = sd['Embedded_text'].apply(clean)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$AAPL needs to run a contest that awards a $14...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[aapl, need, run, contest, award, gift, card, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL update http://stks.co/f062X</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[aapl, update, http]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's Over! Move Along + 1st OPEX of 2014 $SPY ...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[move, along, opex, spy, aapl, amzn, bidu, fb,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what will keep pushing forward 2014?\\n$TSLA $G...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[keep, pushing, forward, tsla, goog, fb, twtr,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Editorial: 2013 was a terrible year for both A...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[editorial, terrible, year, apple, competitor,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Summary of Yesterday's Webcast Featuring $AAPL...</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>[summary, yesterday, webcast, featuring, aapl,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Replying to \\n@SconsetCapital</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>[replying, sconsetcapital]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Early movers: HTZ, FDX, TWTR, NFLX, AAPL &amp; PSX...</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>[early, mover, htz, fdx, twtr, nflx, aapl, psx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>***excellent piece, I agree 100%***\\n@jfahmy\\n...</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>[excellent, piece, agree, jfahmy, blog, post, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Davis Polk on why the SEC rejected $AAPL attem...</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>[davis, polk, sec, rejected, aapl, attempt, ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Embedded_text        Date  \\\n",
       "0    $AAPL needs to run a contest that awards a $14...  2013-12-29   \n",
       "1                    $AAPL update http://stks.co/f062X  2013-12-29   \n",
       "2    It's Over! Move Along + 1st OPEX of 2014 $SPY ...  2013-12-29   \n",
       "3    what will keep pushing forward 2014?\\n$TSLA $G...  2013-12-29   \n",
       "4    Editorial: 2013 was a terrible year for both A...  2013-12-29   \n",
       "..                                                 ...         ...   \n",
       "181  Summary of Yesterday's Webcast Featuring $AAPL...  2013-12-31   \n",
       "182                      Replying to \\n@SconsetCapital  2013-12-31   \n",
       "183  Early movers: HTZ, FDX, TWTR, NFLX, AAPL & PSX...  2013-12-31   \n",
       "184  ***excellent piece, I agree 100%***\\n@jfahmy\\n...  2013-12-31   \n",
       "185  Davis Polk on why the SEC rejected $AAPL attem...  2013-12-31   \n",
       "\n",
       "                                          Cleaned_text  \n",
       "0    [aapl, need, run, contest, award, gift, card, ...  \n",
       "1                                 [aapl, update, http]  \n",
       "2    [move, along, opex, spy, aapl, amzn, bidu, fb,...  \n",
       "3    [keep, pushing, forward, tsla, goog, fb, twtr,...  \n",
       "4    [editorial, terrible, year, apple, competitor,...  \n",
       "..                                                 ...  \n",
       "181  [summary, yesterday, webcast, featuring, aapl,...  \n",
       "182                         [replying, sconsetcapital]  \n",
       "183  [early, mover, htz, fdx, twtr, nflx, aapl, psx...  \n",
       "184  [excellent, piece, agree, jfahmy, blog, post, ...  \n",
       "185  [davis, polk, sec, rejected, aapl, attempt, ex...  \n",
       "\n",
       "[186 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['Cleaned_text'] = sd['Embedded_text'].apply(clean)\n",
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06d77d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Cleaned_text</th>\n",
       "      <th>Untokenized_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$AAPL needs to run a contest that awards a $14...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[aapl, need, run, contest, award, gift, card, ...</td>\n",
       "      <td>aapl need run contest award gift card throwing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL update http://stks.co/f062X</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[aapl, update, http]</td>\n",
       "      <td>aapl update http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's Over! Move Along + 1st OPEX of 2014 $SPY ...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[move, along, opex, spy, aapl, amzn, bidu, fb,...</td>\n",
       "      <td>move along opex spy aapl amzn bidu fb goog lnk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what will keep pushing forward 2014?\\n$TSLA $G...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[keep, pushing, forward, tsla, goog, fb, twtr,...</td>\n",
       "      <td>keep pushing forward tsla goog fb twtr amzn aa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Editorial: 2013 was a terrible year for both A...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[editorial, terrible, year, apple, competitor,...</td>\n",
       "      <td>editorial terrible year apple competitor mediu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Summary of Yesterday's Webcast Featuring $AAPL...</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>[summary, yesterday, webcast, featuring, aapl,...</td>\n",
       "      <td>summary yesterday webcast featuring aapl wynn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Replying to \\n@SconsetCapital</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>[replying, sconsetcapital]</td>\n",
       "      <td>replying sconsetcapital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Early movers: HTZ, FDX, TWTR, NFLX, AAPL &amp; PSX...</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>[early, mover, htz, fdx, twtr, nflx, aapl, psx...</td>\n",
       "      <td>early mover htz fdx twtr nflx aapl psx http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>***excellent piece, I agree 100%***\\n@jfahmy\\n...</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>[excellent, piece, agree, jfahmy, blog, post, ...</td>\n",
       "      <td>excellent piece agree jfahmy blog post everyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Davis Polk on why the SEC rejected $AAPL attem...</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>[davis, polk, sec, rejected, aapl, attempt, ex...</td>\n",
       "      <td>davis polk sec rejected aapl attempt exclude s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Embedded_text        Date  \\\n",
       "0    $AAPL needs to run a contest that awards a $14...  2013-12-29   \n",
       "1                    $AAPL update http://stks.co/f062X  2013-12-29   \n",
       "2    It's Over! Move Along + 1st OPEX of 2014 $SPY ...  2013-12-29   \n",
       "3    what will keep pushing forward 2014?\\n$TSLA $G...  2013-12-29   \n",
       "4    Editorial: 2013 was a terrible year for both A...  2013-12-29   \n",
       "..                                                 ...         ...   \n",
       "181  Summary of Yesterday's Webcast Featuring $AAPL...  2013-12-31   \n",
       "182                      Replying to \\n@SconsetCapital  2013-12-31   \n",
       "183  Early movers: HTZ, FDX, TWTR, NFLX, AAPL & PSX...  2013-12-31   \n",
       "184  ***excellent piece, I agree 100%***\\n@jfahmy\\n...  2013-12-31   \n",
       "185  Davis Polk on why the SEC rejected $AAPL attem...  2013-12-31   \n",
       "\n",
       "                                          Cleaned_text  \\\n",
       "0    [aapl, need, run, contest, award, gift, card, ...   \n",
       "1                                 [aapl, update, http]   \n",
       "2    [move, along, opex, spy, aapl, amzn, bidu, fb,...   \n",
       "3    [keep, pushing, forward, tsla, goog, fb, twtr,...   \n",
       "4    [editorial, terrible, year, apple, competitor,...   \n",
       "..                                                 ...   \n",
       "181  [summary, yesterday, webcast, featuring, aapl,...   \n",
       "182                         [replying, sconsetcapital]   \n",
       "183  [early, mover, htz, fdx, twtr, nflx, aapl, psx...   \n",
       "184  [excellent, piece, agree, jfahmy, blog, post, ...   \n",
       "185  [davis, polk, sec, rejected, aapl, attempt, ex...   \n",
       "\n",
       "                                     Untokenized_clean  \n",
       "0    aapl need run contest award gift card throwing...  \n",
       "1                                     aapl update http  \n",
       "2    move along opex spy aapl amzn bidu fb goog lnk...  \n",
       "3    keep pushing forward tsla goog fb twtr amzn aa...  \n",
       "4    editorial terrible year apple competitor mediu...  \n",
       "..                                                 ...  \n",
       "181  summary yesterday webcast featuring aapl wynn ...  \n",
       "182                            replying sconsetcapital  \n",
       "183        early mover htz fdx twtr nflx aapl psx http  \n",
       "184  excellent piece agree jfahmy blog post everyon...  \n",
       "185  davis polk sec rejected aapl attempt exclude s...  \n",
       "\n",
       "[186 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['Untokenized_clean'] = sd['Cleaned_text'].map(lambda t: \" \".join(t))\n",
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f34e65",
   "metadata": {},
   "source": [
    "# Sentiment analysis using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ac27eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ksnbx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71f70f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Cleaned_text</th>\n",
       "      <th>Untokenized_clean</th>\n",
       "      <th>vader_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$AAPL needs to run a contest that awards a $14...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[aapl, need, run, contest, award, gift, card, ...</td>\n",
       "      <td>aapl need run contest award gift card throwing...</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL update http://stks.co/f062X</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[aapl, update, http]</td>\n",
       "      <td>aapl update http</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's Over! Move Along + 1st OPEX of 2014 $SPY ...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[move, along, opex, spy, aapl, amzn, bidu, fb,...</td>\n",
       "      <td>move along opex spy aapl amzn bidu fb goog lnk...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what will keep pushing forward 2014?\\n$TSLA $G...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[keep, pushing, forward, tsla, goog, fb, twtr,...</td>\n",
       "      <td>keep pushing forward tsla goog fb twtr amzn aa...</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Editorial: 2013 was a terrible year for both A...</td>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>[editorial, terrible, year, apple, competitor,...</td>\n",
       "      <td>editorial terrible year apple competitor mediu...</td>\n",
       "      <td>-0.6369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Embedded_text        Date  \\\n",
       "0  $AAPL needs to run a contest that awards a $14...  2013-12-29   \n",
       "1                  $AAPL update http://stks.co/f062X  2013-12-29   \n",
       "2  It's Over! Move Along + 1st OPEX of 2014 $SPY ...  2013-12-29   \n",
       "3  what will keep pushing forward 2014?\\n$TSLA $G...  2013-12-29   \n",
       "4  Editorial: 2013 was a terrible year for both A...  2013-12-29   \n",
       "\n",
       "                                        Cleaned_text  \\\n",
       "0  [aapl, need, run, contest, award, gift, card, ...   \n",
       "1                               [aapl, update, http]   \n",
       "2  [move, along, opex, spy, aapl, amzn, bidu, fb,...   \n",
       "3  [keep, pushing, forward, tsla, goog, fb, twtr,...   \n",
       "4  [editorial, terrible, year, apple, competitor,...   \n",
       "\n",
       "                                   Untokenized_clean  vader_polarity  \n",
       "0  aapl need run contest award gift card throwing...          0.4215  \n",
       "1                                   aapl update http          0.0000  \n",
       "2  move along opex spy aapl amzn bidu fb goog lnk...          0.0000  \n",
       "3  keep pushing forward tsla goog fb twtr amzn aa...          0.3612  \n",
       "4  editorial terrible year apple competitor mediu...         -0.6369  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['vader_polarity'] = sd['Untokenized_clean'].map(lambda text: sid.polarity_scores(text)['compound'])\n",
    "sd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8354fc",
   "metadata": {},
   "source": [
    "# Calculating each day's sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "394dba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "byday = sd.groupby('Date')['vader_polarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88386ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "byday.to_csv(\"data/twitter_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1399c",
   "metadata": {},
   "source": [
    "# Putting it all into one function\n",
    "It will take a long time to scrape tweets for our 10-year period, so we put the entire process into one function that saves the final sentiment datagram. Each file is considered one batch, with the whole 10-year period being split into 20 batches consisting of 6 months of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "977ae55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_sentiment(begin, end):\n",
    "    # scrape the data\n",
    "    scraped_data = scrape(words=KEYWORDS, since=begin, until=end, interval=1, \n",
    "                      save_images=False, limit=MAX_TWEETS, headless=True, proxy=None, save_dir=ORIGINAL_DIR)\n",
    "    \n",
    "    # keep only relevant columns\n",
    "    sd = scraped_data[['Timestamp', 'Embedded_text']]\n",
    "    \n",
    "    # split into date and time columns\n",
    "    sd[['Date', 'Time']] = sd['Timestamp'].str.split('T', expand=True)\n",
    "\n",
    "    # dropping unnecessary columns and reordering\n",
    "    sd = sd[['Embedded_text', 'Date']]\n",
    "    \n",
    "    # clean the text\n",
    "    sd['Cleaned_text'] = sd['Embedded_text'].apply(clean)\n",
    "\n",
    "    # untokenize the text\n",
    "    sd['Untokenized_clean'] = sd['Cleaned_text'].map(lambda t: \" \".join(t))\n",
    "    \n",
    "    # sentiment analysis\n",
    "    sd['vader_polarity'] = sd['Untokenized_clean'].map(lambda text: sid.polarity_scores(text)['compound'])\n",
    "    \n",
    "    # calculate the mean sentiment by day\n",
    "    byday = sd.groupby('Date')['vader_polarity'].mean()\n",
    "    \n",
    "    name = 'twitter_sentiment_' + '_'.join(keywords) + '_' + st + '_' + en + '.csv'\n",
    "    byday.to_csv(PROCESSED_DIR + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9402f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for curiosity, time each batch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8bfa802b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2022-12-31', '2023-01-1')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the start + end dates of each batch\n",
    "dates = [(\"2022-12-31\", \"2023-01-1\")]\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bce36792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2022-12-31', '2023-01-1')\n",
      "Batch ('2022-12-31', '2023-01-1') elapsed - 3.000563859939575 seconds\n"
     ]
    }
   ],
   "source": [
    "# scrape the data for each batch\n",
    "times = []\n",
    "for date in dates:\n",
    "    start = time.time()\n",
    "    twitter_sentiment(date[0], date[1])\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    times.append(duration)\n",
    "    print(\"Batch \" + str(date) + \" elapsed - \" + str(duration) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7fa1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
