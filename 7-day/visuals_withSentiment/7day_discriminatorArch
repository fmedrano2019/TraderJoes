digraph {
	graph [size="12.45,12.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2094484990448 [label="
 ()" fillcolor=darkolivegreen1]
	2094484971424 [label=MeanBackward0]
	2094484971136 -> 2094484971424
	2094484971136 [label=AddmmBackward0]
	2094484971472 -> 2094484971136
	2095542306592 [label="dense_layer14.bias
 (1)" fillcolor=lightblue]
	2095542306592 -> 2094484971472
	2094484971472 [label=AccumulateGrad]
	2094484971088 -> 2094484971136
	2094484971088 [label=ReluBackward0]
	2094484971184 -> 2094484971088
	2094484971184 [label=MmBackward0]
	2094484970800 -> 2094484971184
	2094484970800 [label=LeakyReluBackward0]
	2094484970656 -> 2094484970800
	2094484970656 [label=MmBackward0]
	2094484970512 -> 2094484970656
	2094484970512 [label=ReshapeAliasBackward0]
	2095664658608 -> 2094484970512
	2095664658608 [label=LeakyReluBackward0]
	2094485012640 -> 2095664658608
	2094485012640 [label=ConvolutionBackward0]
	2094485012736 -> 2094485012640
	2094485012736 [label=LeakyReluBackward0]
	2094485012928 -> 2094485012736
	2094485012928 [label=ConvolutionBackward0]
	2094485013024 -> 2094485012928
	2094485013024 [label=LeakyReluBackward0]
	2094485013216 -> 2094485013024
	2094485013216 [label=ConvolutionBackward0]
	2094485013312 -> 2094485013216
	2094485013312 [label=ConvolutionBackward0]
	2094485013504 -> 2094485013312
	2095542318960 [label="conv_layer0.weight
 (32, 8, 5)" fillcolor=lightblue]
	2095542318960 -> 2094485013504
	2094485013504 [label=AccumulateGrad]
	2094485013456 -> 2094485013312
	2095542318880 [label="conv_layer0.bias
 (32)" fillcolor=lightblue]
	2095542318880 -> 2094485013456
	2094485013456 [label=AccumulateGrad]
	2094485013264 -> 2094485013216
	2095542318800 [label="conv_layer1.weight
 (64, 32, 5)" fillcolor=lightblue]
	2095542318800 -> 2094485013264
	2094485013264 [label=AccumulateGrad]
	2094485013120 -> 2094485013216
	2095542318720 [label="conv_layer1.bias
 (64)" fillcolor=lightblue]
	2095542318720 -> 2094485013120
	2094485013120 [label=AccumulateGrad]
	2094485012976 -> 2094485012928
	2095542318640 [label="conv_layer3.weight
 (128, 64, 5)" fillcolor=lightblue]
	2095542318640 -> 2094485012976
	2094485012976 [label=AccumulateGrad]
	2094485012832 -> 2094485012928
	2095542319680 [label="conv_layer3.bias
 (128)" fillcolor=lightblue]
	2095542319680 -> 2094485012832
	2094485012832 [label=AccumulateGrad]
	2094485012688 -> 2094485012640
	2095542319600 [label="conv_layer6.weight
 (256, 128, 5)" fillcolor=lightblue]
	2095542319600 -> 2094485012688
	2094485012688 [label=AccumulateGrad]
	2094485012544 -> 2094485012640
	2095542306912 [label="conv_layer6.bias
 (256)" fillcolor=lightblue]
	2095542306912 -> 2094485012544
	2094485012544 [label=AccumulateGrad]
	2094484970560 -> 2094484970656
	2094484970560 [label=TBackward0]
	2094484970464 -> 2094484970560
	2095542306832 [label="dense_layer9.weight
 (256, 256)" fillcolor=lightblue]
	2095542306832 -> 2094484970464
	2094484970464 [label=AccumulateGrad]
	2094484970944 -> 2094484971184
	2094484970944 [label=TBackward0]
	2094484970752 -> 2094484970944
	2095542306752 [label="dense_layer12.weight
 (256, 256)" fillcolor=lightblue]
	2095542306752 -> 2094484970752
	2094484970752 [label=AccumulateGrad]
	2094484971232 -> 2094484971136
	2094484971232 [label=TBackward0]
	2094484970704 -> 2094484971232
	2095542306672 [label="dense_layer14.weight
 (1, 256)" fillcolor=lightblue]
	2095542306672 -> 2094484970704
	2094484970704 [label=AccumulateGrad]
	2094484971424 -> 2094484990448
}
