digraph {
	graph [size="12.45,12.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2165440064144 [label="
 ()" fillcolor=darkolivegreen1]
	2165464988304 [label=MeanBackward0]
	2165464988352 -> 2165464988304
	2165464988352 [label=AddmmBackward0]
	2165464988400 -> 2165464988352
	2165311054608 [label="dense_layer14.bias
 (1)" fillcolor=lightblue]
	2165311054608 -> 2165464988400
	2165464988400 [label=AccumulateGrad]
	2165464931584 -> 2165464988352
	2165464931584 [label=ReluBackward0]
	2165464931392 -> 2165464931584
	2165464931392 [label=MmBackward0]
	2165464930768 -> 2165464931392
	2165464930768 [label=LeakyReluBackward0]
	2165464166752 -> 2165464930768
	2165464166752 [label=MmBackward0]
	2165464024640 -> 2165464166752
	2165464024640 [label=ReshapeAliasBackward0]
	2165440078176 -> 2165464024640
	2165440078176 [label=LeakyReluBackward0]
	2165464863792 -> 2165440078176
	2165464863792 [label=ConvolutionBackward0]
	2165443722064 -> 2165464863792
	2165443722064 [label=LeakyReluBackward0]
	2165464464544 -> 2165443722064
	2165464464544 [label=ConvolutionBackward0]
	2165464464208 -> 2165464464544
	2165464464208 [label=LeakyReluBackward0]
	2165464394720 -> 2165464464208
	2165464394720 [label=ConvolutionBackward0]
	2165464395488 -> 2165464394720
	2165464395488 [label=ConvolutionBackward0]
	2165440354192 -> 2165464395488
	2165311055248 [label="conv_layer0.weight
 (32, 8, 5)" fillcolor=lightblue]
	2165311055248 -> 2165440354192
	2165440354192 [label=AccumulateGrad]
	2165440354480 -> 2165464395488
	2165311055168 [label="conv_layer0.bias
 (32)" fillcolor=lightblue]
	2165311055168 -> 2165440354480
	2165440354480 [label=AccumulateGrad]
	2165464394000 -> 2165464394720
	2165311055008 [label="conv_layer1.weight
 (64, 32, 5)" fillcolor=lightblue]
	2165311055008 -> 2165464394000
	2165464394000 [label=AccumulateGrad]
	2165464392512 -> 2165464394720
	2165311054928 [label="conv_layer1.bias
 (64)" fillcolor=lightblue]
	2165311054928 -> 2165464392512
	2165464392512 [label=AccumulateGrad]
	2165464464592 -> 2165464464544
	2165311054448 [label="conv_layer3.weight
 (128, 64, 5)" fillcolor=lightblue]
	2165311054448 -> 2165464464592
	2165464464592 [label=AccumulateGrad]
	2165464462576 -> 2165464464544
	2165311054128 [label="conv_layer3.bias
 (128)" fillcolor=lightblue]
	2165311054128 -> 2165464462576
	2165464462576 [label=AccumulateGrad]
	2165464463680 -> 2165464863792
	2165311054288 [label="conv_layer6.weight
 (256, 128, 5)" fillcolor=lightblue]
	2165311054288 -> 2165464463680
	2165464463680 [label=AccumulateGrad]
	2165464462432 -> 2165464863792
	2165311054208 [label="conv_layer6.bias
 (256)" fillcolor=lightblue]
	2165311054208 -> 2165464462432
	2165464462432 [label=AccumulateGrad]
	2165464024352 -> 2165464166752
	2165464024352 [label=TBackward0]
	2165463911680 -> 2165464024352
	2165311053888 [label="dense_layer9.weight
 (256, 256)" fillcolor=lightblue]
	2165311053888 -> 2165463911680
	2165463911680 [label=AccumulateGrad]
	2165464932160 -> 2165464931392
	2165464932160 [label=TBackward0]
	2165440080816 -> 2165464932160
	2165311054048 [label="dense_layer12.weight
 (256, 256)" fillcolor=lightblue]
	2165311054048 -> 2165440080816
	2165440080816 [label=AccumulateGrad]
	2165464928608 -> 2165464988352
	2165464928608 [label=TBackward0]
	2165443513120 -> 2165464928608
	2165311053968 [label="dense_layer14.weight
 (1, 256)" fillcolor=lightblue]
	2165311053968 -> 2165443513120
	2165443513120 [label=AccumulateGrad]
	2165464988304 -> 2165440064144
}
